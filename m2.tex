\section*{Midterm 2 (Feb 28, 2020)}

\problem{
    f14m2q4
    f15m2q5
}{
    What does ROB stand for, and why is it used in modern advanced pipelines? 
}{
    Reorder Buffer. 

    It can dynamically execute code while maintaining a precise interrupt.
}

\problem{
    f16m2q1
    w19m2q3
    \xxxxxx
}{
    Writes to a cache are inherently slower than reads from a cache - why?
}{
    Read can do check tags and read data in parallel. 
    However, writing is inherently serial/sequential, so we have to check tags before writing the data to the cache. 
}

\problem{
    w12m2q10
    f14m2q1
    f15m2q3
    f16m2q2
    w19m2q1
}{
    What is the primary difference between Tomasulo’s algorithm and Scoreboarding?
}{
    Tomasulo’s: distributed; scoreboarding: centralized.

    \warn{Common data bus(CDB)}
}

\problem{
    f16m2q7
}{
    Why is loop unrolling a valuable optimization technique? What are 2 challenges to using it?
}{
    It allows us to make bigger basic blocks to do code scheduling 

    a) The limited number of registers 
    b) Don't know how to decide how many times to unroll
}

\problem{
    f12m2q10
    f14m2p2
    f15m2q8
    f16m2q8
    w19m2q9
}{
    The book states that slow and wide architectures can be more power-efficient than fast and narrow architectures. 
    Explain why. 
    Also, explain the underlying assumption that is being made, and why it is that we are still making narrow fast machines.
}{
    Slow and wide can lower both clock rate and voltage.
    Lowering $V$ and $f$ means the power goes down since $P = C \cdot f \cdot V^2$.

    If there is enough data-level parallelism, then slow and wide can provide the same throughput as fast and narrow while using less power. 
    There is not always enough DLP though.
}

\problem{
    f15m2q11
    f16m2q10
    w19m2q2
}{
    Briefly outline how a Vector machine works, and what type of parallelism it is exploiting.
}{
    There are several vector registers(VR), fill them up. 
    Single instruction operated on these VRs, such that the single instruction do multiple operations on elements of the VRs. 

    Data level parallelism.
}

\problem{
    f04m1q16
    w10m1q10
    w12f0p1
    f13f0p1
    f14m2p2
    f15m2q12
    f15f0p2
    f15f1p1
    f16m2q11
    f16f0p1
    w19m2q10
}{
    What is the primary difference between superscalar and VLIW processors? 
    Give one advantage and one disadvantage of using each approach. 
    (Compare and contrast VLIW and superscalar with two advantage and one disadvantage.)
    (These have to be different - in other words, if the advantage of superscalar is X, then you can’t say a disadvantage of VLIW is that it can’t do X.)
}{
    \textbf{Superscalar}: hardware does dynamic scheduling  \\
    Pro: better at prediction and disambiguation \& no recompilation on hardware changes \\
    Con: more power consumption \& HW complexity.

    \textbf{VLIW}: complier does static scheduling \\ 
    Pro: simpler hardware, save cost \& power efficient\\
    Con: cannot use runtime information \& \warn{Memory ambiguity}\\
}

\problem{
    f14m2p2
    f14f0p1
    f15m2q9
    f15f1p1
    f16m2q12
    f16f0p1
    w19m2q13
}{
    You have been writing C programs for a simple, non-pipelined machine. 
    You have recently received a promotion, and now your job is to write C programs for a heavily pipelined, high-performance processor. 
    These new programs must execute as fast as possible (the emphasis is on response time, not throughput). 
    Give at least 2 examples of things you should do differently now, and be sure to explain in detail why 
    (what is the problem you are overcoming?)
}{
    \begin{enumerate}
        \item avoid using pointers: pointers screw up register allocation when compiling 
        \item avoid using recursions: return address stack can be overflowed 
        \item inline functions: to make larger basic blocks
    \end{enumerate}

    Avoiding using pointers make the biggest difference in performance, as in general applications pointers are more widely used.
}

\problem{
    f16m2q13
    w20q5q8
}{
    (Dependencies recognization \& register substitution) 
}{
    refer to the exam
}

\problem{
    f16m2q14
}{
    (Dependencies recognization \& NOPS insertion) 
}{
    refer to the exam
}

\problem{
    f16m2q16
}{
    Assuming a 19-bit address and a 256-byte Direct Mapped cache with a linesize=2, show how an address is partitioned/interpreted by the cache.
}{
    offset = 1 bit \\
    256/2 = 128 = $2^7$  entry = 7 bits \\
    tag  = 11 bits
}

\problem{
    f16m2q17
}{
    Assuming an 19-bit address and an 80-byte 10-way Set Associative cache with a linesize=4, show how an address is partitioned/interpreted by the cache.
}{
    offset = 2 bits \\
    80/(10*4) = 2 = $2^1$  set = 1 bit \\
    tag = 16 bits
}

\problem{
    f16m2q18
}{
    Assuming an 19-bit address and a 328-byte Fully Associative cache with a linesize=8, show how an address is partitioned/interpreted by the cache.
}{
    offset = 3 bits \\
    tag = 16 bits
}

\problem{
    f16m2q19
}{
    Given a 1 Megabyte physical memory, a 22-bit Virtual address, and a page size of 1K bytes, write down the number of entries in the Page Table, and the width of each entry.
}{
   1M = $2^{20}$ \\
   1K = $2^{10}$ \\
   $(2^{20})/(2^{10}) = 2^{10} \rightarrow$ 10-bit wide \\
   PM: 10 | 10 \\
   VM: 12 | 10 \\
   $2^{12}$ entries
}

\problem{
    f16m2q20
}{
    Given a 1 Megabyte physical memory, a 34-bit Virtual address, and a page size of 2K bytes, write down the number of entries in the Page Table, and the width of each entry. Is there a problem with this configuration? If so, what is the problem?
}{
    1M = $2^{20}$ \\
    2K = $2^{11}$ \\
    $(2^{20})/(2^{11}) = 2^9 \rightarrow$ 9-bit wide \\
    PM: 9 | 11 \\
    VM: 23 | 11 \\
    $2^{23}$ entries \\
    Problem: It is too large to store in 1M physical memory. We can use two page tables. One is base, another one is secondary.
}

\problem{
    f14m2q5
}{
    Speculation is a very useful technique for improving performance. 
    However, it is not being used as extensively as it once was - why not?
}{
    \todo
}

\problem{
    w19m2q7
}{
    A vector processor is a type of SIMD architecture. 
    What makes it different than a "normal" SIMD machine? 
    (Why does it have a separate section in the SIMD chapter in the book?)
}{
    Vector computers processed the vectors one word at a time through pipelined processors 
    (though still based on a single instruction)
    Whereas modern SIMD computers process all elements of the vector simultaneously.
}