\section*{Week 7 (2/17 - 2/23; Chp.3, Quiz 5)}

\problem{
    w20q5q1
    \xxxxxx
}{
    Assuming a 5-way multiple issue processor with a 9-stage pipeline and no bubbles, how many instructions are in execution at any time?
}{
    $ 5 \times 9 = 45 $
}

\problem{
    w20q5q2
    \xxxxxx
}{
    As of 2011, how many processors combine full speculation with resolving multiple branches per cycle?
}{
    No processor
}

\problem{
    w20q5q3
}{
    Tomasulo's algorithm attempts to achieve something called Data Flow execution. 
    How does the book define Data Flow execution?
}{
    \pg{184} Operation executes as soon as their operands are available.
}


\problem{
    f04m1q16
    w10m1q10
    w12f0p1
    f13f0p2
    f14m2p2
    f15m2q12
    f15f0p1
    f16m2q11
    f16f0p1
    w19m2q10
    w19f0q10
    w19f0q11
    w19f0q12
    w19f0q13
    w19f0q49
    w20q5q4
    w20m2q12
}{
    What is the primary difference between superscalar and VLIW processors? 
    Give one advantage and one disadvantage of using each approach. 
    (Compare and contrast VLIW and superscalar with two advantage and one disadvantage.)
    (These have to be different - in other words, if the advantage of superscalar is X, then you can’t say a disadvantage of VLIW is that it can’t do X.)
}{
    \textbf{Superscalar}: hardware does dynamic scheduling  \\
    Pro: better at prediction and disambiguation \& no recompilation on hardware changes \\
    Con: only have a small window which it can schedule code \& HW complexity and more power consumption.

    \textbf{VLIW}: complier does static scheduling \\ 
    Pro: simpler hardware and clock rate can be faster \& compiler can see the the whole program to do scheduling\\
    Con: cannot use runtime information \& \warn{Memory ambiguity}\\
}

\problem{
    w20q5q5
    \xxxxxx
}{
    The book talks about the limits of ILP and mentions that even when using a perfect model there are some limitations. 
    What are the most important limitations that apply even to the perfect model?
}{
    \pg{220}
    \begin{enumerate}
        \item WAW \& WAR through the memory system
        \item Unnecessary dependencies
        \item Overcoming data flow limits
    \end{enumerate}
}

\problem{
    w15f0p2
}{
    As speculation \blank, power consumption \blank.
}{
    Increases, increases.
}

\problem{
    f13f0p4
    f16m2q5
    f16f0p4
    w19m2q8
    w19f0q32
    w20q5q6
    w20m2q14
}{
    Processors have been built that can issue 8 instructions at a time. 
    However, these pro­cessors are no longer being built - why not? 
    Why would you choose a 3-issue machine over an 8-issue machine?
}{
    Speculation requires a lot of power, and we don't get 8-issue very often because we don't always find that much parallelism, so it is not power efficient.
    I am wasting all of my energy supporting my 8-issue when I only get 3 or 4.

    OR

    8-issue machine has more hardware and does more speculation, and thus requires lots of power. 
    If we don't have enough parallelism available, it will have many bubbles and waste energy 
    Back then we don't care about that, but now the condition has changed and the energy efficiency is more significant now.

}

\problem{
    f12m2q15
    w13f0p1
    w19f0q9
    w20q5q7
}{
    Register renaming is used to avoid name hazards. 
    Is there a technique that can be used to minimize the number of stalls due to true (RAW) hazards? 
    If so, what is it and how does it work?
}{
    Value prediction. 
    Identify when values are being used, and using them in advance when they are needed.
}

\problem{
    f13f0p11
    f16m2q13
    w19f0q48
    w20q5q8
    w20m2q16
}{
    (Dependencies recognization \& register renaming, sampled from w20q5q8) \\
    \nop
}{

    \begin{minipage}[t]{0.49\textwidth}
        a)
        \begin{itemize}
            \item \RAW{1}{4}{F3}  
            \item \WAR{1}{3}{F5}      
            \item \WAR{1}{2}{F7}
            \item \WAR{1}{4}{F7}
            \item \WAR{2}{3}{F5}
            \item \RAW{2}{3}{F7}
            \item \WAW{2}{4}{F7}
            \item \WAR{3}{4}{F7}
        \end{itemize}
    \end{minipage}
    \begin{minipage}[t]{0.49\textwidth}
        b) \\
        15, 14, 13, 7, 11, 5, 9, 4
    
        c)
        \begin{itemize}
            \item \RAW{1}{4}{P7}
            \item \RAW{2}{3}{P6}
        \end{itemize}
    \end{minipage}
}

\problem{
    f13f0p13
    w19f0q50
    w20q5q9
    w20m2q17
}{
    (Fine / Coarse / Simultaneous multithreading, sampled from w20q5q9) \\
    \nop
}{
    \begin{minipage}[t]{0.33\textwidth}
        a)\\
        A A     \\
        A A A   \\
        B       \\
        B       \\
        B B     \\
        B B B   \\
        C C     \\
        C       \\
    \end{minipage}
    \begin{minipage}[t]{0.33\textwidth}
        b)\\
        A A A   \\
        B       \\
        C C     \\
        A A A   \\
        B       \\
        C       \\
    \end{minipage}
    \begin{minipage}[t]{0.33\textwidth}
        c)\\
        A A B C C \\
        A A A B C \\
        B B C C C \\
        A A B B B \\
    \end{minipage}
}

\problem{
    w12m2q10
    w13f0p2
    f14m2q1
    f15m2q3
    f16m2q2
    w19m2q1
    w20m2q2
}{
    What is the primary difference between Tomasulo’s algorithm and Scoreboarding?
}{
    Tomasulo’s: distributed; scoreboarding: centralized.
}

\problem{
    f12m2q10
    f13f0p4
    f14m2p2
    f15m2q8
    f15f0p4
    f16m2q8
    w19m2q9
    w19f0q31
    w20m2q11
}{
    The book states that slow and wide architectures can be more power-efficient than fast and narrow architectures. 
    Explain why. 
    Also, explain the underlying assumption that is being made, and why it is that we are still making narrow fast machines.
}{
    Slow and wide can lower both clock rate and voltage.
    Lowering $V$ and $f$ means the power goes down since $P = C \cdot f \cdot V^2$.

    If there is enough data-level parallelism, then slow and wide can provide the same throughput as fast and narrow while using less power. 
    
    But there is not always enough DLP though and thus slow and wide architectures' throughput is lower than the other's.
}

\problem{
    f13f0q6
    f14m2p2
    f14f0p1
    f15m2q9
    f15f0p8
    f16m2q12
    f16f0p1
    w19m2q13
    w19f0q40
    w20m2q15
}{
    You have been writing C programs for a simple, non-pipelined machine. 
    You have recently received a promotion, and now your job is to write C programs for a heavily pipelined, high-performance processor. 
    These new programs must execute as fast as possible (the emphasis is on response time, not throughput). 
    Give at least 2 examples of things you should do differently now, and be sure to explain in detail why 
    (what is the problem you are overcoming?)
}{
    \begin{enumerate}
        \item avoid using pointers: pointers screw up \textbf{register allocation} when compiling 
        \item inline functions: to make larger basic blocks
    \end{enumerate}

    Avoiding using pointers make the biggest difference in performance, 
    as in general application pointers are more widely used.

}

\problem{
    f14m2q5
    w19f0q27
}{
    Speculation is a very useful technique for improving performance. 
    However, it is not being used as extensively as it once was - why not?
}{
    \warn{
        \pg{201}
        Incorrect speculation does not improve performance; 
        it typically harms performance dramatically and lowers energy efficiency.
    }

    OR
    
    \warn{
        \pg{210}
        \begin{itemize}
            \item Speculation is not free. It takes time and energy, and the recovery of incorrect speculation further reduces performance. 
            \item In addition, to support the higher instruction execution rate needed to benefit from speculation, the processor must have additional resources, which take silicon area and power. 
            \item Finally, if speculation causes an exceptional event to occur, such as a cache or translation lookaside buffer (TLB) miss, the potential for significant performance loss increases, if that event would not have occurred without speculation.
        \end{itemize}
    }
}

\problem{
    f13f0p9
    f13f0p10
    f15f0p9
    f15f0p10
    f16m2q14
    w19f0q46
    w19f0q47
    w20m2q18
}{
    (Dependencies \& NOPs insertion) 
}{
    \todo
}

\problem{
    f13f0p12
    w19f0q45
}{
    (Dependencies \& instruction scheduling) 
}{
    \todo
}